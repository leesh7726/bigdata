{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"작업_2_유형_walkthrough_overkill.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jFeh3GpV5_ZD"},"source":["### 시험 주관사 제공 예시문제: 기계적 처리법\n","##### 도메인 이해, 데이터 이해, 시각화, 모델 기획 단계 skip.\n","##### numerical, categorical 컬럼 처리 또한 기계적으로 진행합니다.\n","* 차후 분석 보고서 작성시 문제가 생길 수 있습니다.(해석력 확보 X)\n","* 실기시험 초단기 패스를 위한 접근법.\n","* 이 방식 적용시 데이터 셋이 바뀌어도\n","* feature / label 재정의 + classifier -> regressor 교환하여 적용 가능."]},{"cell_type":"code","metadata":{"id":"j6O8Ng-p5_ZF","outputId":"c4f3952a-d32d-4087-e42a-796a44ba59c8"},"source":["# feature / label 설정과 분석 방향성 설정이 마무리 되었다고 가정.\n","import warnings\n","import pandas as pd\n","import numpy as np\n","warnings.filterwarnings(\"ignore\")\n","\n","X_train = pd.read_csv('X_train.csv', encoding='euc_kr')\n","y_train = pd.read_csv('y_train.csv', encoding='euc_kr')\n","X_test = pd.read_csv('X_test.csv', encoding='euc_kr')\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3500, 10)\n","(3500, 2)\n","(2482, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"clZTtAmb5_ZG"},"source":["##### 탐색 step"]},{"cell_type":"code","metadata":{"id":"p86Q_hiV5_ZG","outputId":"c2a2beab-3844-4c47-e276-a2ffe812a66d"},"source":["desc_t = X_train.describe(include='all').T\n","desc_t['count']\n","# 환불금액 컬럼은 null 존재 확인 \n","# -> min != 0 이므로 환불 안 한 사람이 null이라 가정 -> fillna(0) 적용\n","# 주구매상품, 주구매지점 컬럼은 categorical 임을 확인 -> one-hot encoding 적용"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["cust_id    3500\n","총구매액       3500\n","최대구매액      3500\n","환불금액       1205\n","주구매상품      3500\n","주구매지점      3500\n","내점일수       3500\n","내점당구매건수    3500\n","주말방문비율     3500\n","구매주기       3500\n","Name: count, dtype: object"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"7yjIjOaP5_ZH","outputId":"2bca882c-9ccc-4102-d550-60acfd0ca5f3"},"source":["num_X_cols = X_train.describe().columns\n","len(num_X_cols)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"nOEMvgjH5_ZI"},"source":["# log-transformation => extreme-positive skew 이므로 변환 적용\n","not_tgt = ['cust_id', '주말방문비율']\n","for col in num_X_cols:\n","    if col in not_tgt : continue\n","    X_train[col] = np.log(X_train[col] + 1)\n","    X_test[col] = np.log(X_test[col] + 1) # log 정의역 고려하여 + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dR0rOQX5_ZI","outputId":"a231cced-18fa-4469-f5ee-26d90ba457ae"},"source":["rows = len(X_train)\n","all_pct = []\n","for col in desc_t.index:\n","    uniq_s = len(np.unique(X_train[col]))\n","    col_uniq_pct = int((uniq_s / rows) * 1000) / 10\n","    all_pct.append({'col' : col, 'unique_counts': uniq_s, 'uniq_pct' : col_uniq_pct})\n","uniq_df = pd.DataFrame(all_pct).sort_values(by='uniq_pct', ascending=False)\n","uniq_df\n","# cust_id 는 PK 성질이므로 feature 반영 X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>col</th>\n","      <th>unique_counts</th>\n","      <th>uniq_pct</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cust_id</td>\n","      <td>3500</td>\n","      <td>100.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>환불금액</td>\n","      <td>3409</td>\n","      <td>97.4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>총구매액</td>\n","      <td>3396</td>\n","      <td>97.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>최대구매액</td>\n","      <td>2576</td>\n","      <td>73.6</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>주말방문비율</td>\n","      <td>1142</td>\n","      <td>32.6</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>내점당구매건수</td>\n","      <td>1107</td>\n","      <td>31.6</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>내점일수</td>\n","      <td>147</td>\n","      <td>4.2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>구매주기</td>\n","      <td>135</td>\n","      <td>3.8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>주구매상품</td>\n","      <td>42</td>\n","      <td>1.2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>주구매지점</td>\n","      <td>24</td>\n","      <td>0.6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       col  unique_counts  uniq_pct\n","0  cust_id           3500     100.0\n","3     환불금액           3409      97.4\n","1     총구매액           3396      97.0\n","2    최대구매액           2576      73.6\n","8   주말방문비율           1142      32.6\n","7  내점당구매건수           1107      31.6\n","6     내점일수            147       4.2\n","9     구매주기            135       3.8\n","4    주구매상품             42       1.2\n","5    주구매지점             24       0.6"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"ZXOjra9V5_ZI"},"source":["X_train = X_train.fillna(0)\n","X_test = X_test.fillna(0) # na 처리"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUX10vRf5_ZJ"},"source":["# Frequency encoding\n","import numpy as np\n","\n","interested = ['주구매상품', '주구매지점']\n","\n","counts = pd.DataFrame(\n","    X_train[interested[0]].value_counts(normalize=True)).reset_index()\n","counts.columns = [interested[0], interested[0]+'_ratio']\n","accu = 0\n","all_accu = []\n","for val in counts[interested[0]+'_ratio']:\n","    accu = accu + val\n","    all_accu.append(accu)\n","counts[interested[0]+'_accu'] = all_accu\n","top_70_1 = list(counts.loc[counts[interested[0]+'_accu'] > 0.7][interested[0]].values)\n","\n","counts = pd.DataFrame(X_train[interested[1]].value_counts(normalize=True)).reset_index()\n","counts.columns = [interested[1], interested[1]+'_ratio']\n","accu = 0\n","all_accu = []\n","for val in counts[interested[1]+'_ratio']:\n","    accu = accu + val\n","    all_accu.append(accu)\n","counts[interested[1]+'_accu'] = all_accu\n","top_70_2 = list(counts.loc[counts[interested[1]+'_accu'] > 0.7][interested[1]].values)\n","\n","# 누적빈출빈도 기준 70% 미만을 other 로 간주\n","X_train[interested[0]] = X_train[interested[0]].apply(\n","    lambda x : x if x in top_70_1 else 'other')\n","X_train[interested[1]] = X_train[interested[1]].apply(\n","    lambda x : x if x in top_70_2 else 'other')\n","X_test[interested[0]] = X_test[interested[0]].apply(\n","    lambda x : x if x in top_70_1 else 'other')\n","X_test[interested[1]] = X_test[interested[1]].apply(\n","    lambda x : x if x in top_70_2 else 'other')\n","\n","num_cols = list(X_train.describe().columns)\n","num_cols.remove('cust_id')\n","X_dataset = [X_train, X_test]\n","for n, dataset in enumerate(X_dataset):\n","    for col in num_cols:\n","        if col == 'cust_id' : continue\n","        freq_name_1 = interested[0] + '_' + col + '_mean'\n","        freq_name_2 = interested[1] + '_' + col + '_mean'\n","        freq_1 = dataset.groupby(interested[0]).agg({col : np.mean}).reset_index()\n","        freq_2 = dataset.groupby(interested[1]).agg({col : np.mean}).reset_index()\n","        freq_1.columns = [interested[0], freq_name_1]\n","        freq_2.columns = [interested[1], freq_name_2]\n","        dataset = pd.merge(dataset, freq_1)\n","        dataset = pd.merge(dataset, freq_2)\n","        X_dataset[n] = dataset\n","        \n","X_train, X_test = X_dataset[0], X_dataset[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpq5wK5L5_ZJ"},"source":["# Target Encoding\n","label = 'gender'\n","\n","all_train = pd.concat([X_train[interested], y_train], axis=1)\n","\n","prod_grouped = all_train.groupby(interested[0]).agg({label : np.mean}).reset_index()\n","prod_grouped.columns = [interested[0], interested[0] + '_target_pct']\n","\n","spot_grouped = all_train.groupby(interested[1]).agg({label : np.mean}).reset_index()\n","spot_grouped.columns = [interested[1], interested[1] + '_target_pct']\n","\n","X_train = pd.merge(X_train, prod_grouped)\n","X_train = pd.merge(X_train, spot_grouped)\n","X_train = X_train.sort_values(by='cust_id').reset_index(drop=True)\n","\n","X_test = pd.merge(X_test, prod_grouped)\n","X_test = pd.merge(X_test, spot_grouped)\n","X_test = X_test.sort_values(by='cust_id').reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"icY7SEya5_ZK"},"source":["# one-hot encoding\n","\n","one_1 = pd.get_dummies(X_train[interested[0]], drop_first=True, prefix=interested[0])\n","one_2 = pd.get_dummies(X_train[interested[1]], drop_first=True, prefix=interested[1])\n","X_train = pd.concat([X_train, one_1, one_2], axis=1)\n","\n","one_1 = pd.get_dummies(X_test[interested[0]], drop_first=True, prefix=interested[0])\n","one_2 = pd.get_dummies(X_test[interested[1]], drop_first=True, prefix=interested[1])\n","X_test = pd.concat([X_test, one_1, one_2], axis=1)\n","\n","train_features = list(X_train.describe().columns)\n","test_features = list(X_test.describe().columns)\n","try:\n","    train_features.remove('cust_id')\n","except:\n","    pass\n","X_train = X_train[train_features]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dnDdOsqp5_ZK"},"source":["# std_scaling\n","from sklearn.preprocessing import RobustScaler as ss\n","scal = ss()\n","X_train_num_cols = list(X_train.describe().columns)\n","X_train_fit = pd.DataFrame(scal.fit_transform(X_train), columns=X_train_num_cols)\n","X_test_num_cols = list(X_test.describe().columns)\n","X_test_fit = pd.DataFrame(scal.fit_transform(X_test[X_test_num_cols]), \n","                          columns=X_test_num_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyYfZ4Cn5_ZL"},"source":["# MINI-SMOTE\n","from sklearn.cluster import KMeans\n","case_one = y_train['gender'].value_counts()[0]\n","case_two = y_train['gender'].value_counts()[1]\n","class_diff = abs(case_one - case_two)\n","prev = 10\n","for i in range(10):\n","    clusters = len(y_train) // (i+1)\n","    after = abs(clusters - class_diff) / class_diff\n","    if prev > after:\n","        prev = after\n","        continue\n","    else:\n","        clusters= len(y_train) // (i)\n","        break\n","model = KMeans(n_clusters=clusters)\n","model.fit(X_train_fit[train_features])\n","cc = pd.DataFrame(model.cluster_centers_, columns=train_features)\n","cc['gender'] = 1\n","all_train = pd.concat([X_train_fit[train_features], y_train[label]], axis=1)\n","all_train = pd.concat([all_train, cc]).reset_index(drop=True)\n","X_train_fit, y_train = all_train[train_features], all_train[label]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbSY2t9v5_ZL"},"source":["##### 변수 선택"]},{"cell_type":"code","metadata":{"id":"eLQZui7G5_ZL","outputId":"566e26a8-3ecd-4c2d-f10f-39eee9d6315a"},"source":["# Wrapper\n","from sklearn.feature_selection import RFE\n","from sklearn.ensemble import RandomForestClassifier as rf\n","estimator = rf(n_jobs=-1, random_state=1)\n","selector = RFE(estimator= estimator)\n","selector.fit(X_train_fit, y_train)\n","rfe_df = pd.DataFrame()\n","select_or_not = pd.DataFrame(sorted(map(\n","    list, zip(selector.support_, X_train_num_cols))), \n","                             columns=['select', 'feature'])\n","rfe_features = list(select_or_not.loc[\n","    select_or_not['select'] == True]['feature'].values)\n","len(rfe_features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ACmJUqGL5_ZM","outputId":"2fc7a5f7-3977-4467-da7d-459a8a7c10fa"},"source":["# Embed\n","estimator = rf(n_jobs=-1, random_state=1)\n","estimator.fit(X_train_fit[rfe_features], y_train)\n","imp = pd.DataFrame()\n","imp['score'] = estimator.feature_importances_\n","imp['feature'] = list(rfe_features)\n","imp = imp.sort_values(by='score', ascending=False)\n","embed_features = list(imp[:len(imp) // 2]['feature'])\n","len(embed_features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"lIzuLkTV5_ZM","outputId":"7399c116-81d5-4c5c-8b2b-4fd0b809beb0"},"source":["'''\n","from sklearn.model_selection import train_test_split\n","\n","def tts(feature_sub):\n","    return train_test_split(X_train_fit[feature_sub], y_train, test_size=0.3, random_state=1)\n","\n","X_train_hold, X_test_hold, y_train_hold, y_test_hold = tts(rfe_features)\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom sklearn.model_selection import train_test_split\\n\\ndef tts(feature_sub):\\n    return train_test_split(X_train_fit[feature_sub], y_train, test_size=0.3, random_state=1)\\n\\nX_train_hold, X_test_hold, y_train_hold, y_test_hold = tts(rfe_features)\\n'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"blOs8pHs5_ZM","outputId":"59eadae3-dfdd-4265-a46b-c15550379277"},"source":["# XAI\n","from sklearn.inspection import permutation_importance as pi\n","perm = pi(estimator, X_train_fit[rfe_features], y_train, random_state=1)\n","perm_df = pd.DataFrame(perm.importances_mean).T\n","perm_df.columns = rfe_features\n","perm_df = perm_df.T.reset_index()\n","perm_df.columns = ['feature', 'perm_imp']\n","perm_features = list(perm_df.sort_values(\n","    by='perm_imp', ascending=False)[:len(perm_df) // 2]['feature'])\n","print(len(perm_features))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T7_gr2RZ5_ZN","outputId":"6fc35b05-612a-4396-d8e9-610dc676c179"},"source":["inter_features = list(set(embed_features).intersection(set(perm_features)))\n","print(len(inter_features))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["17\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qakJdiaE5_ZN"},"source":["##### Model fitting step"]},{"cell_type":"markdown","metadata":{"id":"GjM842bU5_ZN"},"source":["##### Hyperparameter tuning - Random search\n","* Coarse search"]},{"cell_type":"code","metadata":{"id":"4qDSHSPB5_ZN","outputId":"bd4d4210-0c03-469e-ad77-20e4bc9ef846"},"source":["''' Docstring 이용하여 hyperparameter 정의역 확인\n","max_depth : int\n"," |          Maximum tree depth for base learners.\n"," |      learning_rate : float\n"," |          Boosting learning rate (xgb's \"eta\")\n"," |      verbosity : int\n"," |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n"," |      objective : string or callable\n"," |          Specify the learning task and the corresponding learning objective or\n"," |          a custom objective function to be used (see note below).\n"," |      booster: string\n"," |          Specify which booster to use: gbtree, gblinear or dart.\n"," |      tree_method: string\n"," |          Specify which tree method to use.  Default to auto.  If this parameter\n"," |          is set to default, XGBoost will choose the most conservative option\n"," |          available.  It's recommended to study this option from parameters\n"," |          document.\n"," |      n_jobs : int\n"," |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n"," |          algorithms like grid search, you may choose which algorithm to parallelize and\n"," |          balance the threads.  Creating thread contention will significantly slow down both\n"," |          algorithms.\n"," |      gamma : float\n"," |          Minimum loss reduction required to make a further partition on a leaf\n"," |          node of the tree.\n"," |      min_child_weight : float\n"," |          Minimum sum of instance weight(hessian) needed in a child.\n"," |      max_delta_step : float\n"," |          Maximum delta step we allow each tree's weight estimation to be.\n"," |      subsample : float\n"," |          Subsample ratio of the training instance.\n"," |      colsample_bytree : float\n"," |          Subsample ratio of columns when constructing each tree.\n"," |      colsample_bylevel : float\n"," |          Subsample ratio of columns for each level.\n"," |      colsample_bynode : float\n"," |          Subsample ratio of columns for each split.\n"," |      reg_alpha : float (xgb's alpha)\n"," |          L1 regularization term on weights\n"," |      reg_lambda : float (xgb's lambda)\n"," |          L2 regularization term on weights\n"," |      scale_pos_weight : float\n"," |          Balancing of positive and negative weights.\n"," |      base_score:\n"," |          The initial prediction score of all instances, global bias.\n"," |      random_state : int\n","'''\n","from xgboost import XGBClassifier as xg\n","from sklearn.metrics import roc_auc_score as roc\n","from sklearn.model_selection import cross_val_score as cv\n","\n","def hyper_cv_scorer(model):\n","    return np.mean(cv(model, X_train_fit[features], y_train, scoring='roc_auc'))\n","\n","from random import randint\n","import time\n","\n","features = inter_features\n","\n","all_waits = 3600 # 하이퍼파라미터 튜닝에 할애할 총 시간 설정\n","wait_time = all_waits / 3\n","t0 = time.time()\n","results = []\n","while True:\n","    t1 = time.time() - t0\n","    if t1 > wait_time: break\n","    max_depth = randint(2, 100)\n","    n_estimators=randint(100, 300)\n","    learning_rate = randint(1, 999) / 1000\n","    gamma = randint(1, 999) / 1000\n","    min_child_weight = randint(1, 999) / 1000\n","    reg_alpha = randint(1, 999) / 1000\n","    subsample = randint(1, 999) / 1000\n","    colsample_bytree = randint(1, 999) / 1000\n","    model = xg(n_jobs=-1, eval_metric='auc', n_estimators=n_estimators, \n","               reg_alpha=reg_alpha, \n","               max_depth=max_depth, learning_rate=learning_rate, gamma=gamma, \n","               min_child_weight=min_child_weight, use_label_encoder=False, \n","               subsample=subsample, colsample_bytree=colsample_bytree)\n","    \n","    results.append({'score' : hyper_cv_scorer(model), \n","                    'max_depth' : max_depth, 'n_estimators':n_estimators, \n","                    'learning_rate' : learning_rate, \n","                    'gamma' : gamma, 'min_child_weight' : min_child_weight, \n","                    'reg_alpha' : reg_alpha, 'subsample' : subsample, \n","                    'colsample_bytree' : colsample_bytree})\n","    \n","params = pd.DataFrame(results).sort_values(by='score', ascending=False)[:10]\n","param_desc = params.describe()\n","suggestion = param_desc.T[['min', 'max']].T\n","params"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","      <th>max_depth</th>\n","      <th>n_estimators</th>\n","      <th>learning_rate</th>\n","      <th>gamma</th>\n","      <th>min_child_weight</th>\n","      <th>reg_alpha</th>\n","      <th>subsample</th>\n","      <th>colsample_bytree</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>176</th>\n","      <td>0.721365</td>\n","      <td>8</td>\n","      <td>181</td>\n","      <td>0.043</td>\n","      <td>0.171</td>\n","      <td>0.914</td>\n","      <td>0.984</td>\n","      <td>0.272</td>\n","      <td>0.068</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>0.714348</td>\n","      <td>2</td>\n","      <td>165</td>\n","      <td>0.199</td>\n","      <td>0.323</td>\n","      <td>0.448</td>\n","      <td>0.410</td>\n","      <td>0.561</td>\n","      <td>0.429</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>0.714098</td>\n","      <td>2</td>\n","      <td>171</td>\n","      <td>0.104</td>\n","      <td>0.990</td>\n","      <td>0.370</td>\n","      <td>0.215</td>\n","      <td>0.295</td>\n","      <td>0.705</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>0.713289</td>\n","      <td>2</td>\n","      <td>112</td>\n","      <td>0.498</td>\n","      <td>0.842</td>\n","      <td>0.195</td>\n","      <td>0.686</td>\n","      <td>0.845</td>\n","      <td>0.192</td>\n","    </tr>\n","    <tr>\n","      <th>104</th>\n","      <td>0.712615</td>\n","      <td>42</td>\n","      <td>219</td>\n","      <td>0.065</td>\n","      <td>0.635</td>\n","      <td>0.021</td>\n","      <td>0.806</td>\n","      <td>0.272</td>\n","      <td>0.078</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>0.710904</td>\n","      <td>83</td>\n","      <td>186</td>\n","      <td>0.049</td>\n","      <td>0.649</td>\n","      <td>0.479</td>\n","      <td>0.435</td>\n","      <td>0.885</td>\n","      <td>0.084</td>\n","    </tr>\n","    <tr>\n","      <th>267</th>\n","      <td>0.709405</td>\n","      <td>11</td>\n","      <td>119</td>\n","      <td>0.137</td>\n","      <td>0.431</td>\n","      <td>0.088</td>\n","      <td>0.250</td>\n","      <td>0.775</td>\n","      <td>0.113</td>\n","    </tr>\n","    <tr>\n","      <th>158</th>\n","      <td>0.709242</td>\n","      <td>2</td>\n","      <td>202</td>\n","      <td>0.244</td>\n","      <td>0.201</td>\n","      <td>0.958</td>\n","      <td>0.200</td>\n","      <td>0.359</td>\n","      <td>0.291</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.709221</td>\n","      <td>89</td>\n","      <td>141</td>\n","      <td>0.076</td>\n","      <td>0.015</td>\n","      <td>0.804</td>\n","      <td>0.666</td>\n","      <td>0.775</td>\n","      <td>0.041</td>\n","    </tr>\n","    <tr>\n","      <th>169</th>\n","      <td>0.704498</td>\n","      <td>21</td>\n","      <td>219</td>\n","      <td>0.079</td>\n","      <td>0.408</td>\n","      <td>0.839</td>\n","      <td>0.659</td>\n","      <td>0.103</td>\n","      <td>0.116</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        score  max_depth  n_estimators  learning_rate  gamma  \\\n","176  0.721365          8           181          0.043  0.171   \n","58   0.714348          2           165          0.199  0.323   \n","55   0.714098          2           171          0.104  0.990   \n","196  0.713289          2           112          0.498  0.842   \n","104  0.712615         42           219          0.065  0.635   \n","285  0.710904         83           186          0.049  0.649   \n","267  0.709405         11           119          0.137  0.431   \n","158  0.709242          2           202          0.244  0.201   \n","23   0.709221         89           141          0.076  0.015   \n","169  0.704498         21           219          0.079  0.408   \n","\n","     min_child_weight  reg_alpha  subsample  colsample_bytree  \n","176             0.914      0.984      0.272             0.068  \n","58              0.448      0.410      0.561             0.429  \n","55              0.370      0.215      0.295             0.705  \n","196             0.195      0.686      0.845             0.192  \n","104             0.021      0.806      0.272             0.078  \n","285             0.479      0.435      0.885             0.084  \n","267             0.088      0.250      0.775             0.113  \n","158             0.958      0.200      0.359             0.291  \n","23              0.804      0.666      0.775             0.041  \n","169             0.839      0.659      0.103             0.116  "]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"9liZmfkj5_ZP"},"source":["##### Finer_1 Search\n","* suggestion 으로부터 범위 가져와 Finer Search 수행"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"MNPFCoex5_ZR","outputId":"78bfda89-ea4c-403b-d795-2e56d41a8627"},"source":["from random import randint\n","\n","t0 = time.time()\n","results = []\n","while True:\n","    t1 = time.time() - t0\n","    if t1 > wait_time: break\n","    max_depth = randint(suggestion['max_depth']['min'], \n","                        suggestion['max_depth']['max'])\n","    n_estimators = randint(suggestion['n_estimators']['min'], \n","                        suggestion['n_estimators']['max'])\n","    learning_rate = randint(int(suggestion['learning_rate']['min'] * 1000), \n","                            int(suggestion['learning_rate']['max'] * 1000)) / 1000\n","    gamma = randint(int(suggestion['gamma']['min'] * 1000), \n","                    int(suggestion['gamma']['max'] * 1000)) / 1000\n","    min_child_weight = randint(int(suggestion['min_child_weight']['min'] * 1000), \n","                                int(suggestion['min_child_weight']['max'] * 1000)) / 1000\n","    reg_alpha = randint(int(suggestion['reg_alpha']['min'] * 1000), \n","                        int(suggestion['reg_alpha']['max'] * 1000)) / 1000\n","    subsample = randint(int(suggestion['subsample']['min'] * 1000), \n","                        int(suggestion['subsample']['max'] * 1000)) / 1000\n","    colsample_bytree = randint(int(suggestion['colsample_bytree']['min'] * 1000), \n","                        int(suggestion['colsample_bytree']['max'] * 1000)) / 1000\n","    model = xg(n_jobs=-1, eval_metric='auc', n_estimators=n_estimators, \n","               reg_alpha=reg_alpha, \n","               max_depth=max_depth, learning_rate=learning_rate, gamma=gamma, \n","               min_child_weight=min_child_weight, use_label_encoder=False, \n","               subsample=subsample, colsample_bytree=colsample_bytree)\n","    \n","    results.append({'score' : hyper_cv_scorer(model), \n","                    'max_depth' : max_depth, 'n_estimators':n_estimators, \n","                    'learning_rate' : learning_rate, \n","                    'gamma' : gamma, 'min_child_weight' : min_child_weight, \n","                    'reg_alpha' : reg_alpha, 'subsample' : subsample, \n","                    'colsample_bytree' : colsample_bytree})\n","params = pd.DataFrame(results).sort_values(by='score', ascending=False)[:10]\n","param_desc = params.describe()\n","suggestion = param_desc.T[['min', 'max']].T\n","suggestion"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","      <th>max_depth</th>\n","      <th>n_estimators</th>\n","      <th>learning_rate</th>\n","      <th>gamma</th>\n","      <th>min_child_weight</th>\n","      <th>reg_alpha</th>\n","      <th>subsample</th>\n","      <th>colsample_bytree</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>min</th>\n","      <td>0.707432</td>\n","      <td>2.0</td>\n","      <td>119.0</td>\n","      <td>0.061</td>\n","      <td>0.201</td>\n","      <td>0.080</td>\n","      <td>0.36</td>\n","      <td>0.175</td>\n","      <td>0.058</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.724578</td>\n","      <td>53.0</td>\n","      <td>210.0</td>\n","      <td>0.197</td>\n","      <td>0.948</td>\n","      <td>0.934</td>\n","      <td>0.97</td>\n","      <td>0.855</td>\n","      <td>0.391</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        score  max_depth  n_estimators  learning_rate  gamma  \\\n","min  0.707432        2.0         119.0          0.061  0.201   \n","max  0.724578       53.0         210.0          0.197  0.948   \n","\n","     min_child_weight  reg_alpha  subsample  colsample_bytree  \n","min             0.080       0.36      0.175             0.058  \n","max             0.934       0.97      0.855             0.391  "]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"EE6xpJ9L5_ZR"},"source":["##### Finer_2"]},{"cell_type":"code","metadata":{"id":"GFEgXXfc5_ZR","outputId":"c6f87c85-32c6-4211-bed7-179864bfc843"},"source":["t0 = time.time()\n","results = []\n","while True:\n","    t1 = time.time() - t0\n","    if t1 > wait_time: break\n","    max_depth = randint(suggestion['max_depth']['min'], \n","                        suggestion['max_depth']['max'])\n","    n_estimators = randint(suggestion['n_estimators']['min'], \n","                        suggestion['n_estimators']['max'])\n","    learning_rate = randint(int(suggestion['learning_rate']['min'] * 1000), \n","                            int(suggestion['learning_rate']['max'] * 1000)) / 1000\n","    gamma = randint(int(suggestion['gamma']['min'] * 1000), \n","                    int(suggestion['gamma']['max'] * 1000)) / 1000\n","    min_child_weight = randint(int(suggestion['min_child_weight']['min'] * 1000), \n","                                int(suggestion['min_child_weight']['max'] * 1000)) / 1000\n","    reg_alpha = randint(int(suggestion['reg_alpha']['min'] * 1000), \n","                        int(suggestion['reg_alpha']['max'] * 1000)) / 1000\n","    subsample = randint(int(suggestion['subsample']['min'] * 1000), \n","                        int(suggestion['subsample']['max'] * 1000)) / 1000\n","    colsample_bytree = randint(int(suggestion['colsample_bytree']['min'] * 1000), \n","                        int(suggestion['colsample_bytree']['max'] * 1000)) / 1000\n","    model = xg(n_jobs=-1, eval_metric='auc', n_estimators=n_estimators, \n","               reg_alpha=reg_alpha, \n","               max_depth=max_depth, learning_rate=learning_rate, gamma=gamma, \n","               min_child_weight=min_child_weight, use_label_encoder=False, \n","               subsample=subsample, colsample_bytree=colsample_bytree)\n","    \n","    results.append({'score' : hyper_cv_scorer(model), \n","                    'max_depth' : max_depth, 'n_estimators':n_estimators, \n","                    'learning_rate' : learning_rate, \n","                    'gamma' : gamma, 'min_child_weight' : min_child_weight, \n","                    'reg_alpha' : reg_alpha, 'subsample' : subsample, \n","                    'colsample_bytree' : colsample_bytree})\n","params = pd.DataFrame(results).sort_values(by='score', ascending=False)[:1]\n","params"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","      <th>max_depth</th>\n","      <th>n_estimators</th>\n","      <th>learning_rate</th>\n","      <th>gamma</th>\n","      <th>min_child_weight</th>\n","      <th>reg_alpha</th>\n","      <th>subsample</th>\n","      <th>colsample_bytree</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>259</th>\n","      <td>0.732913</td>\n","      <td>3</td>\n","      <td>209</td>\n","      <td>0.089</td>\n","      <td>0.349</td>\n","      <td>0.482</td>\n","      <td>0.602</td>\n","      <td>0.776</td>\n","      <td>0.084</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        score  max_depth  n_estimators  learning_rate  gamma  \\\n","259  0.732913          3           209          0.089  0.349   \n","\n","     min_child_weight  reg_alpha  subsample  colsample_bytree  \n","259             0.482      0.602      0.776             0.084  "]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"kNpj3X-05_ZS"},"source":["##### Final model"]},{"cell_type":"code","metadata":{"id":"nmx09GNt5_ZS","outputId":"95ee1a6e-2dd7-42fe-8349-7f9c83410c73"},"source":["final_model = xg(n_jobs=-1, eval_metric='auc', \n","                 n_estimators=params['n_estimators'].values[0], \n","                 reg_alpha=params['reg_alpha'].values[0], \n","                 max_depth=params['max_depth'].values[0], \n","                 learning_rate=params['learning_rate'].values[0], \n","                 gamma=params['gamma'].values[0], \n","                 min_child_weight=params['min_child_weight'].values[0],\n","                 subsample=params['subsample'].values[0], \n","                 colsample_bytree=params['colsample_bytree'].values[0], \n","                 use_label_encoder=False)\n","\n","final_model.fit(X_train_fit[features], y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=0.084, eval_metric='auc',\n","              gamma=0.349, gpu_id=-1, importance_type='gain',\n","              interaction_constraints=None, learning_rate=0.089,\n","              max_delta_step=0, max_depth=3, min_child_weight=0.482,\n","              missing=nan, monotone_constraints=None, n_estimators=209,\n","              n_jobs=-1, num_parallel_tree=1, random_state=0, reg_alpha=0.602,\n","              reg_lambda=1, scale_pos_weight=1, subsample=0.776,\n","              tree_method=None, use_label_encoder=False,\n","              validate_parameters=False, verbosity=None)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"_MAkUn6H5_ZS"},"source":["##### make submit_file"]},{"cell_type":"code","metadata":{"id":"H90r4E595_ZS","outputId":"34b1b3fc-9489-4ce6-ae9a-9dc26cd0b6c1"},"source":["interested = ['cust_id', 'gender']\n","for_submit = X_test.copy()\n","for_submit['gender'] = pd.DataFrame(final_model.predict_proba(\n","    X_test_fit[features]), columns=['no_use', 'gender'])['gender']\n","for_submit[interested].to_csv('submit.csv', index=False)\n","for_submit[interested].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cust_id</th>\n","      <th>gender</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3500</td>\n","      <td>0.613101</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3501</td>\n","      <td>0.187606</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3502</td>\n","      <td>0.317750</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3503</td>\n","      <td>0.574963</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3504</td>\n","      <td>0.627964</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   cust_id    gender\n","0     3500  0.613101\n","1     3501  0.187606\n","2     3502  0.317750\n","3     3503  0.574963\n","4     3504  0.627964"]},"metadata":{"tags":[]},"execution_count":21}]}]}